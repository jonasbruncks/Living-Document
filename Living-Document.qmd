title: "Living Document - [Your Project Title]"
author:
  
"Moritz Eckmann"
"Permallim Spahija"
"Jonas Bruncks"
date: "r format(Sys.Date(), '%B %d, %Y')"
format:
  html:
    toc: true
    toc-depth: 3
  pdf:
    documentclass: article
    papersize: a4
    fontsize: 12pt
bibliography: references.bib
csl: apa-7.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE


_quarto/
.quarto/
*.html
*.pdf


### Code of Conduct

#### Umgang mit Feedback, unterschiedlichen Perspektiven und Meinungsverschiedenheiten

#### Faire Aufteilung der Arbeitslast

#### Verhalten in Bezug auf vereinbarte und verpflichtende Termine

#### Einhaltung der wissenschaftlichen Integrität (siehe vorangehende Folien)

#### Verpflichtung zum Schutz von Daten und zur Wahrung ihrer Vertraulichkeit

#### Nutzung und Kennzeichnung von AI Tools (z.B. ChatGPT)


# 1. Einletung

# 2. Literaturübersicht

Die drei Studien NEBULA zur KI-Erkennung von Fake News, Forschung zu visueller Desinformation und die Übersichtsstudie zu Gesundheits-Fake News zeigen grundsätzlich alle das gleiche Problem. Falschinformationen verbreiten sich heutzutage viel schneller und weiter als korrekte Informationen. Das liegt daran, dass emotionale Inhalte, Social Media-Algorithmen und geschlossene Meinungsblasen die Verbreitung fördern und vor allem keine anderen Meinungen und Informationen zulassen.

Das NEBULA Projekt entwickelt zwar gute technische Lösungen zum Erkennen von Falschinformationen, da die Menge an Mis- bzw. Desinformationen aber so groß ist, reicht die Technik noch nicht aus um die Masse an den fragwürdigen Informationen als gesichert falsch einzustufen. Betroffene können sowohl Einzelpersonen als auch die breite Bevölkerung sein. Dadurch kann der Ruf von einzelnen Menschen zu Unrecht geschädigt werden oder eine allgemeine Unsicherheit entstehen, die unter anderem auch die Demokratie gefährden kann. Die Forschung zu Bildern und Videos zeigt in erster Linie ein Problem: So genannte Deepfakes und Cheapfakes. Videos mit Inhalten, die so nie geschehen sind, oder von Personen durchgeführt werden, die in der Realität nicht beteiligte waren. Diese Art von Fehlinformationen wirken durch die schnelle Verbreitung in den sozialen Medien sowie dem visuellen Erlebnis stärker als reine Textnachrichten und umgehen die kritische Filter. Genau wie in der Gesundheitsstudie wird deutlich, dass persönliche Geschichten und Verschwörungstheorien sich besonders gut in sozialen Medien verbreiten. Was können wir dagegen tun? Wir brauchen mehrere Ansätze gleichzeitig. Auf technischer Ebene zum Beispiel Tools wie NEBULA. Diese Art von Werkzeugen können Plattformen helfen Quellen besser zu kennzeichnen und fragwürdige Themen bzw. Informationen zu kennzeichnen. Auf psychologischer Ebene ist es wichtig Menschen zu zeigen, wie Manipulation funktioniert und wie man sich davor schützen kann. Der kritische Blick auf neue Informationen kann oft schon stutzig machen und die Verbreitung eindämmen. Die politische Aufgabe sollte eine stärkere Regulation der sozialen Medien sowie eine verbreitete Aufklärung über Mis- und Fehlinformationen sein. Noch offen sind die Langfristige Wirkung der Maßnahmen, die Verbindung mit den jeweiligen Zielgruppen und die Unterscheidung zwischen Textwarnungen und visuellen Hinweisen. Das Wichtigste ist, dass technische Lösungen, psychologische Ansätze und politische Regulierung in Zukunft kombiniert werden können. Zum Beispiel könnten KI-Tools nicht nur warnen, sondern mit Erklärvideos direkt wichtiges Informationen zu diesem Thema vermitteln.


# 3. Methode

# 4. Ergebnisse

# 5. Diskussion
